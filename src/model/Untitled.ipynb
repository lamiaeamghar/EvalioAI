{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e99a000-0e7f-4549-be32-d02396b99c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cost_function, optimize_model, plot_error_distribution \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the cleaned dataset\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\INOCOD\\Advanced EvalioIA\\src\\model\\optimization.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integer, Real\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55ea780-caf4-41bd-accb-d617748859ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-optimize in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-optimize) (25.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.5.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (24.1)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557bb552-d760-4838-a8e9-6f532b837452",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cost_function, optimize_model, plot_error_distribution\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultilingual_support\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assistant\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchat_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RealEstateChatbot\n",
      "File \u001b[1;32m~\\Documents\\INOCOD\\Advanced EvalioIA\\src\\model\\optimization.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integer, Real\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from optimization import cost_function, optimize_model, plot_error_distribution\n",
    "from multilingual_support import assistant\n",
    "from chat_interface import RealEstateChatbot\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the cleaned dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df, target_column):\n",
    "    \"\"\"\n",
    "    Preprocess data by:\n",
    "    1. Adding new features\n",
    "    2. Separating features and target\n",
    "    3. Splitting into train/test sets\n",
    "    4. Creating preprocessing pipelines\n",
    "    \"\"\"\n",
    "    # Add new feature\n",
    "    df['prix_per_surface'] = df['prix'] / df['surface']\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Drop redundant columns if needed\n",
    "    if 'chambres' in numerical_cols and 'bedrooms' in numerical_cols:\n",
    "        X = X.drop(columns=['chambres', 'salles_de_bains'])\n",
    "        numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Create preprocessing transformers\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    # Bundle preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Split data into training and test sets (80%/20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "def build_model(preprocessor):\n",
    "    \"\"\"Build and return a machine learning pipeline\"\"\"\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train the model and evaluate its performance\"\"\"\n",
    "    print(\"\\nTraining the model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "    train_r2 = r2_score(y_train, train_preds)\n",
    "    test_r2 = r2_score(y_test, test_preds)\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Testing RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Training RÂ²: {train_r2:.4f}\")\n",
    "    print(f\"Testing RÂ²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Analyze residuals\n",
    "    residuals = test_preds - y_test\n",
    "    large_errors = residuals[np.abs(residuals) > 200000]  # Example threshold\n",
    "    if len(large_errors) > 0:\n",
    "        print(f\"\\nLarge prediction errors found: {len(large_errors)} samples\")\n",
    "    \n",
    "    return model, test_preds, residuals\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    \"\"\"Save the trained model to disk\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        joblib.dump(model, file_path)\n",
    "        print(f\"\\nModel saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "def generate_conversation(predicted_price: float, features: list) -> str:\n",
    "    \"\"\"Handles multilingual conversations about predictions\"\"\"\n",
    "    print(\"\\nAvailable languages: English (en), French (fr), Arabic (ar)\")\n",
    "    user_input = input(\"Ask about the prediction (type 'exit' to end): \")\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        return \"Conversation ended\"\n",
    "    \n",
    "    context = {\n",
    "        'predicted_price': predicted_price,\n",
    "        'important_features': features[:3]  # Top 3 features\n",
    "    }\n",
    "    \n",
    "    return assistant.generate_response(user_input, context)\n",
    "def chat_interface(model_path: str):\n",
    "    \"\"\"Run interactive chat prediction\"\"\"\n",
    "    bot = RealEstateChatbot(model_path)\n",
    "    \n",
    "    print(\"\\nðŸ  Real Estate Chatbot (type 'quit' to exit)\")\n",
    "    print(\"Example input: '3 bedroom apartment with 120mÂ² in Casablanca'\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            break\n",
    "            \n",
    "        price = bot.predict_from_text(user_input)\n",
    "        if price:\n",
    "            context = {\n",
    "                'predicted_price': price,\n",
    "                'input_features': bot.extract_features(user_input)\n",
    "            }\n",
    "            response = assistant.generate_response(user_input, context)\n",
    "            print(f\"AI: {response}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    data_path = r'C:\\Users\\hp\\Documents\\INOCOD\\Advanced EvalioIA\\data\\data_cleaned_no_outliers.csv'\n",
    "    model_path = r'C:\\Users\\hp\\Documents\\INOCOD\\Advanced EvalioIA\\models\\real_estate_model.joblib'\n",
    "    target_column = 'prix'\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(data_path)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Preprocess data and split into train/test\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocess_data(df, target_column)\n",
    "    \n",
    "    print(\"\\nData Split Summary:\")\n",
    "    print(f\"Training set size: {len(X_train)} samples\")\n",
    "    print(f\"Test set size: {len(X_test)} samples\")\n",
    "    \n",
    "    # Build and optimize model\n",
    "    print(\"\\nOptimizing model hyperparameters...\")\n",
    "    optimized_model = optimize_model(X_train, y_train, preprocessor, n_iter=30)  # Increased iterations\n",
    "    \n",
    "    # Train and evaluate final model\n",
    "    trained_model, test_predictions, residuals = train_and_evaluate(\n",
    "        optimized_model, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Calculate and print cost\n",
    "    train_cost = cost_function(trained_model, X_train, y_train)\n",
    "    test_cost = cost_function(trained_model, X_test, y_test)\n",
    "    print(f\"\\nTraining Cost: {train_cost:.2f}\")\n",
    "    print(f\"Testing Cost: {test_cost:.2f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(trained_model, model_path)\n",
    "    \n",
    "    # Create a DataFrame with actual and predicted values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': test_predictions,\n",
    "        'Difference': residuals,\n",
    "        'Percentage_Difference': (residuals / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSample Predictions vs Actual:\")\n",
    "    print(results_df.head(10).to_string())\n",
    "    \n",
    "    # Save results for analysis\n",
    "    results_df.to_csv(os.path.join(os.path.dirname(model_path), 'prediction_results.csv'), index=False)\n",
    "    print(f\"\\nPrediction results saved to {os.path.join(os.path.dirname(model_path), 'prediction_results.csv')}\")\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plot_error_distribution(y_test, test_predictions)\n",
    "    \n",
    "    # Plot feature importances if using RandomForest\n",
    "    if hasattr(trained_model.named_steps['regressor'], 'feature_importances_'):\n",
    "        try:\n",
    "            feature_names = []\n",
    "            feature_names.extend(X_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "            ohe = trained_model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "            cat_features = ohe.get_feature_names_out(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "            feature_names.extend(cat_features)\n",
    "            \n",
    "            importances = trained_model.named_steps['regressor'].feature_importances_\n",
    "            indices = np.argsort(importances)[-10:]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title('Top 10 Feature Importances')\n",
    "            plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "            plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Relative Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCould not plot feature importances: {e}\")\n",
    "    \n",
    "    # Add conversation after predictions\n",
    "    # while True:\n",
    "    #     response = generate_conversation(\n",
    "    #         predicted_price=test_predictions[0],  # First prediction\n",
    "    #         features=list(X_train.columns[:3])  # First 3 features\n",
    "    #     )\n",
    "    #     print(\"\\nAI:\", response)\n",
    "        \n",
    "    #     if \"Conversation ended\" in response:\n",
    "    #         break\n",
    "\n",
    "    # After saving the model:\n",
    "    if input(\"\\nStart chat interface? (y/n): \").lower() == 'y':\n",
    "        chat_interface(model_path, list(X_train.columns))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3632fab-af7d-48b1-8db3-9c618392f38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-optimize in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-optimize) (25.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.5.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (24.1)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc14ba4c-bcd3-4b1c-be94-d6afbc09aff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acc649b1-c250-4b7d-9e97-391da4296e9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cost_function, optimize_model, plot_error_distribution \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the cleaned dataset\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\INOCOD\\Advanced EvalioIA\\src\\model\\optimization.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integer, Real\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from optimization import cost_function, optimize_model, plot_error_distribution \n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the cleaned dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df, target_column):\n",
    "    \"\"\"\n",
    "    Preprocess data by:\n",
    "    1. Separating features and target\n",
    "    2. Splitting into train/test sets\n",
    "    3. Creating preprocessing pipelines\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Create preprocessing transformers\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    # Bundle preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Split data into training and test sets (80%/20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "def build_model(preprocessor):\n",
    "    \"\"\"Build and return a machine learning pipeline\"\"\"\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1)) \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train the model and evaluate its performance\"\"\"\n",
    "    # Train the model\n",
    "    print(\"\\nTraining the model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "    train_r2 = r2_score(y_train, train_preds)\n",
    "    test_r2 = r2_score(y_test, test_preds)\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Testing RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Training RÂ²: {train_r2:.4f}\")\n",
    "    print(f\"Testing RÂ²: {test_r2:.4f}\")\n",
    "    \n",
    "    return model, test_preds\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    \"\"\"Save the trained model to disk\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        joblib.dump(model, file_path)\n",
    "        print(f\"\\nModel saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    data_path = r'C:\\Users\\hp\\Documents\\INOCOD\\Advanced EvalioIA\\data\\data_cleaned_no_outliers.csv'\n",
    "    model_path = r'C:\\Users\\hp\\Documents\\INOCOD\\Advanced EvalioIA\\models\\real_estate_model.joblib'\n",
    "    target_column = 'prix'\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(data_path)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Preprocess data and split into train/test\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocess_data(df, target_column)\n",
    "    \n",
    "    print(\"\\nData Split Summary:\")\n",
    "    print(f\"Training set size: {len(X_train)} samples\")\n",
    "    print(f\"Test set size: {len(X_test)} samples\")\n",
    "    \n",
    "    # Build and optimize model\n",
    "    print(\"\\nOptimizing model hyperparameters...\")\n",
    "    optimized_model = optimize_model(X_train, y_train, preprocessor)\n",
    "    \n",
    "    # Train and evaluate final model\n",
    "    trained_model, test_predictions = train_and_evaluate(\n",
    "        optimized_model, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Calculate and print cost\n",
    "    train_cost = cost_function(trained_model, X_train, y_train)\n",
    "    test_cost = cost_function(trained_model, X_test, y_test)\n",
    "    print(f\"\\nTraining Cost: {train_cost:.2f}\")\n",
    "    print(f\"Testing Cost: {test_cost:.2f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(trained_model, model_path)\n",
    "    \n",
    "    # Create a DataFrame with actual and predicted values\n",
    "    results_df = pd.DataFrame({\n",
    "        'titre': X_test['titre'],\n",
    "        'surface': X_test['surface'],\n",
    "        'location': X_test['location'],\n",
    "        'city': X_test['city'],\n",
    "        'Actual': y_test,\n",
    "        'Predicted': test_predictions,\n",
    "        'Difference': test_predictions - y_test,\n",
    "        'Percentage_Difference': ((test_predictions - y_test) / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSample Predictions vs Actual:\")\n",
    "    print(results_df.head(10))\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plot_error_distribution(y_test, test_predictions)\n",
    "    \n",
    "    # Plot feature importances if using RandomForest\n",
    "    if hasattr(trained_model.named_steps['regressor'], 'feature_importances_'):\n",
    "        try:\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = []\n",
    "            # Numerical features\n",
    "            feature_names.extend(X_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "            # Categorical features (after one-hot encoding)\n",
    "            ohe = trained_model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "            cat_features = ohe.get_feature_names_out(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "            feature_names.extend(cat_features)\n",
    "            \n",
    "            importances = trained_model.named_steps['regressor'].feature_importances_\n",
    "            indices = np.argsort(importances)[-10:]  # Top 10 features\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title('Top 10 Feature Importances')\n",
    "            plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "            plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Relative Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCould not plot feature importances: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b29d27-d714-42ea-b5f1-cc087bf91de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
